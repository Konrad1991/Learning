{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30433,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"library(tensorflow)\nlibrary(keras)\nmnist <- dataset_mnist()\ntrain_images <- mnist$train$x \ntrain_labels <- mnist$train$y\ntest_images <- mnist$test$x\ntest_labels <- mnist$test$y\n\nmodel <- keras_model_sequential(list(\n layer_dense(units = 512, activation = \"relu\"),\n layer_dense(units = 10, activation = \"softmax\")\n))\ncompile(model,\n       optimizer = \"rmsprop\",\n       loss = \"sparse_categorical_crossentropy\",\n       metrics = \"accuracy\")\n\ntrain_images <- array_reshape(train_images, c(60000, 28 * 28))\ntrain_images <- train_images / 255\ntest_images <- array_reshape(test_images, c(10000, 28 * 28))\ntest_images <- test_images / 255\n\nhistory <- fit(model, train_images, train_labels, epochs = 5, batch_size = 128)\nplot(history)\ntest_digits <- test_images[1:10, ]\npredictions <- predict(model, test_digits)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(tensorflow)\nlibrary(keras)\n\nlayer_naive_dense <- function(input_size, output_size, activation) {\n  self <- new.env(parent = emptyenv())\n  attr(self, \"class\") <- \"NaiveDense\"\n\n  self$activation <- activation\n\n  w_shape <- c(input_size, output_size)\n  w_initial_value <- array(rnorm(input_size * output_size, mean = 0, sd = 1e-1), dim = w_shape)\n\n  self$W <- tf$Variable(w_initial_value)\n\n  b_shape <- c(output_size)\n  b_initial_value <- array(0, b_shape) \n  self$b <- tf$Variable(b_initial_value)\n\n  self$weights <- list(self$W, self$b)\n\n  self$call <- function(inputs) {\n    self$activation(tf$matmul(inputs, self$W) + self$b)\n  }\n\n  self\n}\n\nnaive_model_sequential <- function(layers) {\n  self <- new.env(parent = emptyenv())\n  attr(self, \"class\") <- \"NaiveSequential\"\n\n  self$layers <- layers\n\n  weights <- lapply(layers, function(layer) layer$weights)\n  self$weights <- do.call(c, weights)\n\n  self$call <- function(inputs) {\n    x <- inputs\n    for (layer in self$layers)\n    x <- layer$call(x)\n    x\n  }\n\n  self\n}\n                    \nmodel <- naive_model_sequential(list(\n layer_naive_dense(input_size = 28 * 28, output_size = 512,\n                   activation = tf$nn$relu),\n layer_naive_dense(input_size = 512, output_size = 10,\n                   activation = tf$nn$softmax)\n))\nstopifnot(length(model$weights) == 4)\n                    \nnew_batch_generator <- function(images, labels, batch_size = 128) {\n  self <- new.env(parent = emptyenv())\n  attr(self, \"class\") <- \"BatchGenerator\"\n\n  stopifnot(nrow(images) == nrow(labels))\n  self$index <- 1\n  self$images <- images\n  self$labels <- labels\n  self$batch_size <- batch_size\n  self$num_batches <- ceiling(nrow(images) / batch_size)\n\n  self$get_next_batch <- function() {\n    start <- self$index\n    if(start > nrow(images))\n      return(NULL)\n\n    end <- start + self$batch_size - 1\n    if(end > nrow(images))\n      end <- nrow(images)\n\n    self$index <- end + 1\n    indices <- start:end\n    list(images = self$images[indices, ],\n      labels = self$labels[indices])\n  }\n\n  self\n}\n                    \nlearning_rate <- 1e-3\n\nupdate_weights <- function(gradients, weights) {\n  stopifnot(length(gradients) == length(weights))\n  for (i in seq_along(weights))\n    weights[[i]]$assign_sub(\n      gradients[[i]] * learning_rate)\n}\n                    \noptimizer <- optimizer_sgd(learning_rate = 1e-3)\n\nzip_lists <- function(a, b) {\n    stopifnot(length(a) == length(b))\n    Map(c, a, b)\n}                    \n                    \nupdate_weights <- function(gradients, weights) \n    optimizer$apply_gradients(zip_lists(gradients, weights))\n\none_training_step <- function(model, images_batch, labels_batch) {\n    with(tf$GradientTape() %as% tape, {\n        predictions <- model$call(images_batch)\n        per_sample_losses <- loss_sparse_categorical_crossentropy(labels_batch, predictions)\n        average_loss <- mean(per_sample_losses)\n    })\n    gradients <- tape$gradient(average_loss, model$weights)\n    update_weights(gradients, model$weights)\n    average_loss\n}\n                    \nfit <- function(model, images, labels, epochs, batch_size = 128) {\n for (epoch_counter in seq_len(epochs)) {\n   cat(\"Epoch \", epoch_counter, \"\\n\")\n   batch_generator <- new_batch_generator(images, labels)\n   for (batch_counter in seq_len(batch_generator$num_batches)) {\n     batch <- batch_generator$get_next_batch()\n     loss <- one_training_step(model, batch$images, batch$labels)\n     if (batch_counter %% 100 == 0)\n         print(paste(\n         \"loss at batch \", loss, \" counter\", batch_counter \n         ))\n   }\n }\n}\n                    \n                    \nmnist <- dataset_mnist()\ntrain_images <- array_reshape(mnist$train$x, c(60000, 28 * 28)) / 255\ntest_images <- array_reshape(mnist$test$x, c(10000, 28 * 28)) / 255\ntest_labels <- mnist$test$y\ntrain_labels <- mnist$train$y\nfit(model, train_images, train_labels, epochs = 20, batch_size = 128)\n                    \npredictions <- as.array(model$call(test_images))\npred_labels <- max.col(predictions) - 1\nmatches <- pred_labels == test_labels\nprint(paste(\"accuracy: \", mean(matches) ) )","metadata":{"execution":{"iopub.status.busy":"2023-05-15T13:53:32.321750Z","iopub.execute_input":"2023-05-15T13:53:32.324261Z","iopub.status.idle":"2023-05-15T13:59:13.482947Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch  1 \n[1] \"loss at batch  2.3132080963572  counter 100\"\n[1] \"loss at batch  1.96473986034839  counter 200\"\n[1] \"loss at batch  1.8407879928024  counter 300\"\n[1] \"loss at batch  1.54412988496258  counter 400\"\nEpoch  2 \n[1] \"loss at batch  1.40883322402089  counter 100\"\n[1] \"loss at batch  1.11925147592799  counter 200\"\n[1] \"loss at batch  1.25245706617174  counter 300\"\n[1] \"loss at batch  1.03790369219984  counter 400\"\nEpoch  3 \n[1] \"loss at batch  1.05048981761004  counter 100\"\n[1] \"loss at batch  0.803862034206925  counter 200\"\n[1] \"loss at batch  0.996266653528907  counter 300\"\n[1] \"loss at batch  0.812011601747603  counter 400\"\nEpoch  4 \n[1] \"loss at batch  0.866955316726097  counter 100\"\n[1] \"loss at batch  0.65152636524896  counter 200\"\n[1] \"loss at batch  0.855192180455102  counter 300\"\n[1] \"loss at batch  0.686102490085561  counter 400\"\nEpoch  5 \n[1] \"loss at batch  0.756903824467251  counter 100\"\n[1] \"loss at batch  0.56394397827812  counter 200\"\n[1] \"loss at batch  0.764398719065539  counter 300\"\n[1] \"loss at batch  0.605726374633929  counter 400\"\nEpoch  6 \n[1] \"loss at batch  0.683693870814425  counter 100\"\n[1] \"loss at batch  0.507605355557318  counter 200\"\n[1] \"loss at batch  0.699867933983002  counter 300\"\n[1] \"loss at batch  0.549675293459289  counter 400\"\nEpoch  7 \n[1] \"loss at batch  0.631506043555375  counter 100\"\n[1] \"loss at batch  0.468520211737357  counter 200\"\n[1] \"loss at batch  0.650938579327006  counter 300\"\n[1] \"loss at batch  0.508056438745255  counter 400\"\nEpoch  8 \n[1] \"loss at batch  0.59211271997282  counter 100\"\n[1] \"loss at batch  0.439881339801822  counter 200\"\n[1] \"loss at batch  0.612115143911441  counter 300\"\n[1] \"loss at batch  0.475923076606958  counter 400\"\nEpoch  9 \n[1] \"loss at batch  0.561297074156242  counter 100\"\n[1] \"loss at batch  0.417838414501307  counter 200\"\n[1] \"loss at batch  0.580356780990815  counter 300\"\n[1] \"loss at batch  0.45029822454377  counter 400\"\nEpoch  10 \n[1] \"loss at batch  0.536418933549808  counter 100\"\n[1] \"loss at batch  0.400307332736893  counter 200\"\n[1] \"loss at batch  0.55371047722891  counter 300\"\n[1] \"loss at batch  0.429283212397125  counter 400\"\nEpoch  11 \n[1] \"loss at batch  0.515917804096345  counter 100\"\n[1] \"loss at batch  0.385991643037673  counter 200\"\n[1] \"loss at batch  0.530990598703297  counter 300\"\n[1] \"loss at batch  0.411648481835283  counter 400\"\nEpoch  12 \n[1] \"loss at batch  0.498711695370515  counter 100\"\n[1] \"loss at batch  0.374052897514984  counter 200\"\n[1] \"loss at batch  0.511414456295963  counter 300\"\n[1] \"loss at batch  0.396409354345122  counter 400\"\nEpoch  13 \n[1] \"loss at batch  0.483939918245208  counter 100\"\n[1] \"loss at batch  0.363932583022398  counter 200\"\n[1] \"loss at batch  0.494290469147614  counter 300\"\n[1] \"loss at batch  0.383099083430512  counter 400\"\nEpoch  14 \n[1] \"loss at batch  0.471126834521581  counter 100\"\n[1] \"loss at batch  0.355256188690884  counter 200\"\n[1] \"loss at batch  0.479149239771339  counter 300\"\n[1] \"loss at batch  0.371488037025797  counter 400\"\nEpoch  15 \n[1] \"loss at batch  0.459868689297009  counter 100\"\n[1] \"loss at batch  0.347681153851694  counter 200\"\n[1] \"loss at batch  0.465640927912032  counter 300\"\n[1] \"loss at batch  0.36112558186655  counter 400\"\nEpoch  16 \n[1] \"loss at batch  0.44988326774657  counter 100\"\n[1] \"loss at batch  0.341013346804071  counter 200\"\n[1] \"loss at batch  0.45348068617216  counter 300\"\n[1] \"loss at batch  0.351777718163593  counter 400\"\nEpoch  17 \n[1] \"loss at batch  0.440882542401046  counter 100\"\n[1] \"loss at batch  0.335065184807752  counter 200\"\n[1] \"loss at batch  0.442489567410198  counter 300\"\n[1] \"loss at batch  0.343310641099502  counter 400\"\nEpoch  18 \n[1] \"loss at batch  0.432777563127949  counter 100\"\n[1] \"loss at batch  0.32970471502677  counter 200\"\n[1] \"loss at batch  0.432506094035512  counter 300\"\n[1] \"loss at batch  0.335606126886108  counter 400\"\nEpoch  19 \n[1] \"loss at batch  0.425412857846181  counter 100\"\n[1] \"loss at batch  0.324824955656956  counter 200\"\n[1] \"loss at batch  0.423369008771158  counter 300\"\n[1] \"loss at batch  0.328533610877982  counter 400\"\nEpoch  20 \n[1] \"loss at batch  0.418663637810366  counter 100\"\n[1] \"loss at batch  0.320343794784618  counter 200\"\n[1] \"loss at batch  0.414955841052509  counter 300\"\n[1] \"loss at batch  0.321960156241107  counter 400\"\n[1] \"accuracy:  0.8993\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Binary classification\n\n- Based on [Essential math for DS](https://www.oreilly.com/library/view/essential-math-for/9781098102920/)","metadata":{}},{"cell_type":"code","source":"# Import data\nlibrary(rsample)\n\ndf <- read.csv(\"https://tinyurl.com/y2qmhfsr\")\nknitr::kable(head(df))\nsplit <- initial_split(df, prop = 3/4)\ntrain_data <- training(split)\ntest_data <- testing(split)\n\nx_train <- train_data[, 1:3] /255\nx_test <- test_data[, 1:3] /255\ny_train <- train_data[, 4]\ny_test <- test_data[, 4]\nn <- dim(x_train)[1]\n\nx_train <- as.matrix(x_train)\nx_test <- as.matrix(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Forward propagation\nw_hidden <- matrix(runif(9), nrow = 3, ncol = 3)\nw_output <- matrix(runif(3), nrow = 1, ncol = 3)\n\nb_hidden <- matrix(runif(3), nrow = 3, ncol = 1)\nb_output <- matrix(runif(1), nrow = 1, ncol = 1)\n\nrelu <- function(x) {\n    for(i in seq_along(x)) {\n        if(x[i] < 0) {\n            x[i] = 0\n        }\n    }\n    return(x)\n}\n\nlogistic <- function(x) {\n    1 / (1 + exp(-x))\n}\n\nforward_prop <- function(x) {\n    x <- t(x)\n    z1 <- w_hidden %*% x + b_hidden[,1]\n    a1 <- relu(z1)\n    z2 <- w_output %*% a1 + b_output[1, 1]\n    a2 <- logistic(z2)\n    return(list(z1, a1, z2, a2))\n}\n\ntest_pred <- forward_prop(x_test)[[4]]\ntest_comparison <- ifelse(ifelse(test_pred > 0.5, 1, 0) == y_test, 1, 0)\naccuracy <- sum(test_comparison) / length(y_test)\naccuracy","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backward propagation\nbackward_prop <- function(z1, a1, z2, a2, x, y) {\n  x <- as.matrix(x)\n  dc_da2 <- 2*a2 - 2*y\n  da2_dz2 <- exp(-z2) / (1 + exp(-z2))^2\n  dz2_da1 <- w_output\n  dz2_dw2 <- a1\n  dz2_db2 <- 1\n  da1_dz1 <- z1 > 0\n  dz1_dw1 <- x\n  dz1_db1 <- 1\n  \n  dc_dw2 <- (dc_da2 %*% t(da2_dz2))[1,1] * dz2_dw2\n  dc_db2 <- dc_da2 %*% t(da2_dz2) * dz2_db2\n  dc_da1 <- dc_da2 %*% t(da2_dz2) %*% dz2_da1\n  dc_dw1 <- (dc_da1 %*% da1_dz1)[1, 1] * t(dz1_dw1)\n  dc_db1 <- dc_da1 %*% da1_dz1 * dz1_db1\n  return(list(dc_dw2, dc_db1, dc_dw2, dc_db2))\n}\n\nres <- forward_prop(x_test)\nz1 <- res[[1]]\na1 <- res[[2]]\nz2 <- res[[3]]\na2 <- res[[4]]\ntest <- backward_prop(z1, a1, z2, a2, x_test[1, ], y_test[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L <- 0.05\nfor(i in seq(1, 100000)) {\n    idx <- sample(1:n, 1, replace = FALSE)\n    x_sample <- t(x_train[idx, ])\n    y_sample <- y_train[idx]\n    res <- forward_prop(x_sample)\n    z1 <- res[[1]]\n    a1 <- res[[2]]\n    z2 <- res[[3]]\n    a2 <- res[[4]]\n    res <- backward_prop(z1, a1, z2, a2, x_sample, y_sample)\n    dw1 <- res[[1]]\n    db1 <- res[[2]]\n    dw2 <- res[[3]]\n    db2 <- res[[4]]\n    w_hidden <- w_hidden - L * dw1[,1]\n    b_hidden <- b_hidden - L * db1[1,1]\n    w_output <- w_output - L * dw2[,1]\n    b_output <- b_output - L * db2[1,1]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred <- forward_prop(x_test)[[4]]\ntest_comparison <- ifelse(ifelse(test_pred > 0.5, 1, 0) == y_test, 1, 0)\naccuracy <- sum(test_comparison) / length(y_test)\naccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict iris species\n\n- this is really not working.\n- Maybe there is somewhere a bug","metadata":{}},{"cell_type":"code","source":"rm(list = ls())\npd <- function(x) {\n  d <- dim(x)\n  if(!is.null(d)) {\n    print(d)\n  } else {\n    print(length(x))\n  }\n}\n\nrelu <- function(x) {\n  for(i in seq_along(x)) {\n    if(x[i] < 0) {\n      x[i] = 0\n    }\n  }\n  return(x)\n}\n\nrelu_derivative <- function(x) {\n  x[x <= 0] <- 0\n  x[x > 0] <- 1\n  return(x)\n}\n\nsoftmax <- function(x) {\n  exp(x) / sum(exp(x))\n}\n\nsoftmax_derivative <- function(x) {\n  p <- softmax(x)\n  diag(p) - tcrossprod(p)\n}\n\n# Predict iris species\ndf <- iris\nn <- dim(df)[1]\ndf <- df[sample(1:n, n, replace = FALSE),]\ntrain_data <- df[1:round(n*0.75), ]\ntest_data <- df[round(n*0.75):n, ]\n\nx_train <- train_data[, 1:4]\nx_test <- test_data[, 1:4]\nmeans <- apply(x_train, 2, mean)\nsds <- apply(x_train, 2, sd)\nfor(i in 1:4) {\n  x_train[, i] <- (x_train[, i] - means[i]) / sds[i]\n  x_test[, i] <- (x_test[, i] - means[i]) / sds[i]\n}\n\ny_train <- train_data[, 5] \ny_test <- test_data[, 5]\n\nx_train <- as.matrix(x_train)\nx_test <- as.matrix(x_test)\ny_train <- sapply(y_train, function(x) {\n  if(x == \"setosa\") {\n    return(c(1, 0, 0))\n  } else if(x == \"versicolor\") {\n    return(c(0, 1, 0))\n  } else {\n    return(c(0, 0, 1))\n  }\n})\ny_test <- sapply(y_test, function(x) {\n  if(x == \"setosa\") {\n    return(0)\n  } else if(x == \"versicolor\") {\n    return(1)\n  } else {\n    return(2)\n  }\n})\n\nw_hidden <- matrix(runif(16), nrow = 4, ncol = 4) # 4x4\nw_output <- matrix(runif(12), nrow = 3, ncol = 4) # 3x4\n\nb_hidden <- matrix(runif(4), nrow = 4, ncol = 1) # 4x1\nb_output <- matrix(runif(3), nrow = 1, ncol = 3) # 1x3\n\n# x_train 112x4\n# x_test 39x4\n# y_train 3x112\n# y_test 39\n\nforward_prop <- function(x) {\n  z1 <- w_hidden %*% x + b_hidden[,1] # 4x4 %*% 1x4 + 4x1\n  a1 <- relu(z1)\n  z2 <- w_output %*% a1 + b_output[1, ]\n  a2 <- softmax(z2)\n  return(list(z1, a1, z2, a2))\n}\n\ntest_pred <- apply(x_test, 1, forward_prop)\ntest_pred <- lapply(test_pred, function(x) x[[4]])\ntest_pred <- do.call(cbind, test_pred)\ntest_comparison <- apply(test_pred, 2, function(x) {\n  return(which.max(x) - 1)\n})\ntest_comparison <- ifelse( (test_comparison == y_test) == TRUE, 0, 1)\naccuracy <- sum(test_comparison) / length(y_test)\naccuracy\n\nbackward_prop <- function(z1, a1, z2, a2, x, y) {\n  x <- as.matrix(x)\n  # z1 4x1\n  # a1 4x1\n  # z2 3x1\n  # a2 3x1\n  # x 4x1\n  # y 3\n  dc_da2 <-  a2 - y  # 3x1 - 3 = 3x1\n  da2_dz2 <- softmax_derivative(z2) # 3x1 = 3x3\n  dz2_da1 <- w_output #3x4\n  dz2_dw2 <- a1 # 4x1\n  dz2_db2 <- 1 \n  da1_dz1 <- relu_derivative(z1) # 4x1 = 4x1\n  dz1_dw1 <- x # 4x1\n  dz1_db1 <- 1\n  dc_dw2 <- t(t(dc_da2) %*% da2_dz2) %*% t(dz2_dw2) # 3x4 --> correct\n  dc_db2 <- t(t(dc_da2) %*% da2_dz2) * dz2_db2 # 3x1 --> correct\n  dc_da1 <- t(dz2_da1) %*% dc_da2 * da1_dz1 # 4x1\n  dc_dw1 <- outer(dc_da1[,1] * da1_dz1[,1], x[,1]) # 4x4 --> correct\n  dc_db1 <- dc_da1 # 4x1 --> correct\n  return(list(dc_dw1, dc_db1, dc_dw2, dc_db2)) \n}\n\ntest_backprop <- function() {\n  res <- forward_prop(x_test[1,])\n  z1 <- as.matrix(res[[1]])\n  a1 <- as.matrix(res[[2]])\n  z2 <- as.matrix(res[[3]])\n  a2 <- as.matrix(res[[4]])\n  test <- backward_prop(z1, a1, z2, a2, x_test[1, ], y_test[1])\n}\ntest_backprop()\n\nL <- 0.05\nn <- dim(x_train)[1]\nfor(i in seq(1, 20000)) {\n  idx <- sample(1:n, 1, replace = FALSE)\n  x_sample <- (x_train[idx, ])\n  y_sample <- y_train[, idx]\n  res <- forward_prop(x_sample)\n  z1 <- res[[1]]\n  a1 <- res[[2]]\n  z2 <- res[[3]]\n  a2 <- res[[4]]\n  res <- backward_prop(z1, a1, z2, a2, x_sample, y_sample)\n  dw1 <- res[[1]] # 4x4\n  db1 <- res[[2]] # 4x1\n  dw2 <- res[[3]] # 3x4\n  db2 <- res[[4]] # 3x1\n  \n  w_hidden <- w_hidden - L * dw1\n  b_hidden <- b_hidden - L * db1[,1]\n  w_output <- w_output - L * dw2\n  b_output <- b_output - L * db2[, 1]\n}\n\ntest_pred <- apply(x_test, 1, forward_prop)\ntest_pred <- lapply(test_pred, function(x) x[[4]])\ntest_pred <- do.call(cbind, test_pred)\ntest_comparison <- apply(test_pred, 2, function(x) {\n  return(which.max(x) - 1)\n})\ntest_comparison <- ifelse( (test_comparison == y_test) == TRUE, 0, 1)\naccuracy <- sum(test_comparison) / length(y_test)\naccuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verification via keras\nlibrary(keras)\nlibrary(tensorflow)\n\nmodel <- keras_model_sequential(input_shape = c(4)) %>%\n  layer_flatten() %>%\n  layer_dense(1, activation = \"relu\") %>%\n  layer_dropout(0.2) %>%\n  layer_dense(3)\n\nloss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = loss_fn,\n  metrics = \"accuracy\"\n)\n\n\ndf <- iris\nn <- dim(df)[1]\ndf <- df[sample(1:n, n, replace = FALSE),]\ntrain_data <- df[1:round(n*0.75), ]\ntest_data <- df[round(n*0.75):n, ]\nx_train <- train_data[, 1:4]\nx_test <- test_data[, 1:4]\nmeans <- apply(x_train, 2, mean)\nsds <- apply(x_train, 2, sd)\nfor(i in 1:4) {\n  x_train[, i] <- (x_train[, i] - means[i]) / sds[i]\n  x_test[, i] <- (x_test[, i] - means[i]) / sds[i]\n}\ny_train <- as.integer(train_data[, 5]) - 1\ny_test <- as.integer(test_data[, 5]) - 1\nx_train <- array(stack(x_train)$values, dim = dim(x_train))\nx_test <- array(stack(x_test)$values, dim = dim(x_test))\nhistory <- model %>% fit(x_train, y_train, epochs = 250)\nplot(history)\nmodel %>% evaluate(x_test,  y_test, verbose = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}